# -*- coding: utf-8 -*-
"""Insurance prediction by Sayam

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rAIctXS42bfxVGp5qxyKDzwZv_IGMGy3
"""

import numpy as np

import matplotlib.pyplot as plt

import pandas as pd
import sys

from google.colab import files
uploaded = files.upload()

dataset = pd.read_csv('insurance.csv')
dataset.head()

dataset.info()

dataset.describe()

dataset.isnull().sum()

dataset.hist(figsize=(10,10))
plt.show()

continuous_cols = [col for col in dataset.columns if dataset[col].nunique()>15]
print(continuous_cols)

matrix = dataset[continuous_cols].corr()
mask = np.triu(np.ones_like(matrix, dtype=bool))
sns.heatmap(matrix,mask=mask,annot=True)

num_lst = []
cat_lst = []

from pandas.api.types import is_string_dtype, is_numeric_dtype

for column in dataset:
    plt.figure(column, figsize = (5,5))
    plt.title(column)
    if is_numeric_dtype(dataset[column]):
        dataset[column].plot(kind = 'hist')
        num_lst.append(column)
    elif is_string_dtype(dataset[column]):
        dataset[column].value_counts().plot(kind = 'bar')
        cat_lst.append(column)

print(num_lst)
print(cat_lst)

print(dataset.isnull().sum().sum())

X = dataset.iloc[:, :-1].values #returns all rows and first column
y = dataset.iloc [:, -1].values #returns all rows and last column

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X[:, 1] = le.fit_transform(X[:, 1])
X[:, 4] = le.fit_transform(X[:, 4])

np.set_printoptions(threshold=sys.maxsize)
print(X[:5,:10])

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [5])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

np.set_printoptions(threshold=sys.maxsize)
print(X[:5,:10])

from sklearn.model_selection import train_test_split
train_X,test_X,train_y,test_y=train_test_split(X,y,test_size=0.2,random_state=0)

from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=2)
X_poly = poly_reg.fit_transform(train_X)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_poly,train_y)

y_pred = model.predict(poly_reg.fit_transform(test_X))

np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), test_y.reshape(len(test_y),1)),1)[:10])

plt.subplots(figsize=(15,6))

# Age vs Expenses
plt.subplot(1,2,1)
plt.scatter(dataset['age'], y, color = 'red')
plt.scatter(dataset['age'], model.predict(poly_reg.fit_transform(X)), color = 'blue')
plt.title('Actual Expenses and Predicted Expenses', fontsize = 16)
plt.xlabel('Age', fontsize = 14)
plt.ylabel('Expenses',fontsize = 14)

# BMI vs Expenses
plt.subplot(1,2,2)
plt.scatter(dataset['bmi'], y, color = 'red')
plt.scatter(dataset['bmi'], model.predict(poly_reg.fit_transform(X)), color = 'blue')
plt.title('Actual Expenses and Predicted Expenses', fontsize = 16)
plt.xlabel('BMI', fontsize = 14)
plt.ylabel('Expenses',fontsize = 14)
plt.show()

from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
print("MSE : ",mean_squared_error(y_pred,test_y))
print("MAE : ",mean_absolute_error(y_pred,test_y))
print("R Score :",r2_score(y_pred,test_y))

import seaborn as sns

sns.countplot(x='region',data=dataset)

sns.scatterplot(data=dataset, x="expenses", y="region")

f = plt.figure(figsize=(10,3))
f.add_subplot(121)
sns.countplot(x='smoker',data=dataset)
f.add_subplot(122)
sns.scatterplot(data=dataset, x="expenses", y="smoker")

f = plt.figure(figsize=(10,3))
f.add_subplot(121)
sns.countplot(x='children',data=dataset)
f.add_subplot(122)
sns.scatterplot(data=dataset, x="expenses", y="children")

plt.subplot(1,1,1)
sns.countplot(x='sex',data=dataset)

f = plt.figure(figsize=(10,3))
f.add_subplot(121)
sns.lineplot(x='age',y='expenses',data=dataset)
f.add_subplot(122)
sns.lineplot(x='bmi',y='expenses',data=dataset)

sns.lineplot(x='bmi',y='expenses',data=dataset)

import matplotlib.pyplot as plt
corr = dataset.corr()
# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

sns.pairplot(dataset)

X = dataset[['bmi','age','smoker','children']]
Y = dataset['expenses']

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train,X_test,y_train,y_test = sklearn.model_selection.train_test_split(X,Y,test_size=0.25)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

fig, axes = plt.subplots(ncols = 3, figsize = (15,6), squeeze=True)
plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=None)
dataset.plot(kind='scatter', x='age', y='expenses', ax=axes[0])
dataset.plot(kind='scatter', x='children', y='expenses', ax=axes[1])
dataset.plot(kind='scatter', x='bmi', y='expenses', ax=axes[2])

fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (15,10))

dataset.plot(kind='hist', y='age', ax=axes[0][0], color = 'blue')
dataset.plot(kind='hist', y='bmi', ax=axes[0][1], color = 'orange', bins = 54)
dataset.plot(kind='hist', y='children', ax=axes[1][0], color = 'red', bins = 6)
dataset.plot(kind='hist', y='expenses', ax=axes[1][1], color = 'green', bins = 80)

palette=['#EB5050','#3EA2FF']
fig, axes = plt.subplots(ncols = 3, figsize = (15,6), squeeze=True)
sns.scatterplot(x='bmi', y='expenses', ax=axes[0], data=dataset,hue='sex', palette=palette)
sns.scatterplot(x='bmi', y='expenses', ax=axes[1], data=dataset,hue='smoker', palette=palette)
sns.scatterplot(x='bmi', y='expenses', ax=axes[2], data=dataset,hue='region')

fig, axes = plt.subplots(ncols=3, figsize = (15,6))
dataset['sex'].value_counts().plot(kind='bar', color = 'orange', ax=axes[0],title="Sex", legend = 'sex')
dataset['region'].value_counts().plot(kind='bar', color = 'green', ax=axes[1],title="Region", legend = 'region')
dataset['smoker'].value_counts().plot(kind='bar', ax=axes[2],title="Smoker", legend = 'smoker')

palette=['#EB5050','#3EA2FF']
sns.catplot(x='sex', y='expenses', kind='violin', palette=palette, data=dataset)

palette=['#EB5050','#2DFFAB']
sns.catplot(x='sex', y='expenses', kind='violin', hue='smoker', palette=palette, data=dataset)

from scipy import stats
from scipy.stats import norm
fig =plt.figure(figsize=(18,6))
plt.subplot(1,2,1)
sns.distplot(dataset['expenses'], fit=norm)
(mu,sigma)= norm.fit(dataset['expenses'])
plt.legend(['For Normal dist. mean: {:.2f} | std: {:.2f}'.format(mu,sigma)])
plt.ylabel('Frequency')
plt.title('Distribution of expenses')

palette=['#EB5050','#2DFFAB']
sns.set(style="ticks")
sns.pairplot(data=dataset, hue='smoker', palette=palette)

dataset.head()

# Changing binary categories to 1s and 0s
dataset['sex'] = dataset['sex'].map(lambda s :1  if s == 'female' else 0)
dataset['smoker'] = dataset['smoker'].map(lambda s :1  if s == 'yes' else 0)
dataset.head()

X = dataset.drop(['expenses'], axis = 1)
y = dataset.expenses
print('Shape of X: ', X.shape)
print('Shape of y: ', y.shape)

dataset['children'].value_counts()

fig, ax = plt.subplots(1,2, figsize=(14,8))
sns.boxplot(x = 'sex',y='expenses', data = dataset, ax=ax[0])
ax[0].set_title('Sex vs expenses')
sns.boxplot(x = 'children',y='expenses', data = dataset, ax=ax[1])
ax[1].set_title('Number of children vs expenses')

"""GRAPHICALLY EXPLORING THE CORRELATION BETWEEN DIFFERENT FEATURES WITH MEDICAL EXPENSES VIA REGRESSION PLOTS"""

fig1, ax1 = plt.subplots(1,4, figsize=(14,8))
sns.regplot(x = 'age',y='expenses', data = dataset, ax=ax1[0],color='green')
ax1[0].set_title('Age vs expenses ')
sns.regplot(x = 'bmi',y='expenses', data = dataset, ax=ax1[1],color='red')
ax1[1].set_title('BMI vs expenses')
sns.regplot(x = 'smoker',y='expenses', data = dataset, ax=ax1[2])
ax1[2].set_title('Smoker vs expenses')
sns.regplot(x = 'children',y='expenses', data = dataset, ax=ax1[3],color='orange')
ax1[3].set_title('Number of Children vs expenses')

# Commented out IPython magic to ensure Python compatibility.
import xgboost as xgb
# %matplotlib inline
from scipy import stats
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

parameters= [{'alpha': [0.001,0.1,1, 10, 100, 1000,10000,100000,100000],'normalize':[True,False]} ]

df = dataset

dff=df

dff['sex']=dff['sex'].apply(lambda x:1 if x=='male' else 0)

dff

dff.corr()['expenses'].sort_values(ascending=False)

X=dff[['bmi','smoker','age','sex','children']]

Y=dff['expenses']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

df.corr().drop('expenses',axis=0)['expenses'].sort_values(ascending=False)

fig=plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), annot=True,robust=True)

df.dtypes

df_gptest = df[['smoker','sex','region','expenses']]

df_gptest

grouped_test=df_gptest[['smoker', 'expenses']].groupby(['smoker'])
grouped_test.head(2)

grouped_test2=df_gptest[['region', 'expenses']].groupby(['region'])

f_val, p_val = stats.f_oneway(grouped_test2.get_group('northeast')['expenses'], grouped_test2.get_group('northwest')['expenses'], grouped_test2.get_group('southeast')['expenses'],grouped_test2.get_group('southwest')['expenses'])
print( "ANOVA results: F=", f_val, ", P =", p_val)

dff['smoker']=dff['smoker'].apply(lambda x:1 if x=='yes' else 0)

dff['region']=dff['region'].apply(lambda x:1 if x=='southeast' else 0)

grouped_test2=df_gptest[['sex', 'expenses']].groupby(['sex'])

"""RIDGE REGRESSION"""

RR=Ridge()
RR

Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
Grid = GridSearchCV(RR, parameters ,cv=4)

Grid = GridSearchCV(RR, parameters ,cv=4)

pr2=PolynomialFeatures(degree=6)
x_train_pr=pr.fit_transform(X_train)
x_test_pr=pr.fit_transform(X_test)

Grid.fit(x_train_pr, y_train)

BestRR=Grid.best_estimator_
BestRR

RR_train_r2_score = BestRR.score(x_train_pr, y_train)
RR_train_r2_score

RR_r2_score=BestRR.score(x_test_pr, y_test)
RR_r2_score

rr_pred=BestRR.predict(x_test_pr)

RR_rmse = np.sqrt(mean_squared_error(y_test,rr_pred))
RR_rmse

dataset['region'].value_counts()

Rsqu_test = []

order = [num for num in range(1,10)]
for n in order:
    pr = PolynomialFeatures(degree=n)

lr = LinearRegression()

lr.fit(X_train,y_train)

Y_hat = lr.predict(X_test)

lr_train_r2_score=lr.score(X_train,y_train)
lr.score(X_train,y_train)

lr_r2_score = r2_score(y_test,Y_hat)
lr_r2_score

lr_rmse=np.sqrt(mean_squared_error(y_test,Y_hat))
lr_rmse

data_dmatrix = xgb.DMatrix(data=X,label=Y)

params = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],
'colsample_bytree':[i/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}

# Initialize XGB and GridSearch
xgb2 = xgb.XGBRegressor(nthread=-1)

grid = GridSearchCV(xgb2, params)
grid.fit(X_train, y_train)

"""# ***CALCULATING THE R2 SCORE OF THE XGBOOST REGRESSION MODEL ON TRAINING AND TESTING DATA***"""

xgb_train_r2_score = r2_score(y_train, grid.best_estimator_.predict(X_train))
xgb_train_r2_score

xgb_r2_score = r2_score(y_test, grid.best_estimator_.predict(X_test))
xgb_r2_score

"""# ***CALCULATING THE ROOT MEAN SQUARE ERROR ON THE TEST SET OF THE XGBOOST REGRESSION MODEL***"""

xgb_rmse = np.sqrt(mean_squared_error(y_test,grid.best_estimator_.predict(X_test)))
xgb_rmse

"""# ***NEURAL NETWORK REGRESSION:***

# ***`NORMALISING THE DATA FOR USE BY NEURAL NETWORK`***
"""

min_max_scaler = MinMaxScaler()

X_train_norm=min_max_scaler.fit_transform(X_train)

X_test_norm=min_max_scaler.fit_transform(X_test)

model=Sequential()

X_train.shape

"""# ***`CREATING THE NETWORK BY ADDING 4 LAYERS OF 4 NEURONS EACH AND RECTIFIED LINEAR UNIT AS THE ACTIVATION FUNCTION`***"""

model.add(Dense(4,activation="relu"))
model.add(Dense(4,activation="relu"))
model.add(Dense(4,activation="relu"))
model.add(Dense(4,activation="relu"))

model.add(Dense(1))

model.compile(optimizer="adam",loss="mse")

"""# ***`FITTING THE MODEL WITH THE NORMALISED TRAINING DATA, TRAINING FOR 5000 EPOCHS`***"""

model.fit(x=X_train_norm, y=y_train, validation_data=(X_test_norm,y_test), batch_size=128,epochs=5000)

losses=pd.DataFrame(model.history.history)
losses

"""# ***`
PLOTTING THE TRAINING SET AND TESTING LOSS AS A FUNCTION OF THE NUMBER OF EPOCHS`***
"""

losses.plot(figsize=(14,8))

nn_pred = model.predict(X_test_norm)

nn_train_r2_score = r2_score(y_train,model.predict(X_train_norm))
nn_train_r2_score

nn_r2_score = r2_score(y_test,nn_pred)
nn_r2_score

nn_rmse = np.sqrt(mean_squared_error(y_test,nn_pred))
nn_rmse

print("                                         EVALUATION OF THE 4 MODELS:")
print()
print()
print("LINEAR REGRESSION:")
print()
print("r2_score on training data: "+str(lr_train_r2_score))
print("r2_score on test set: "+str(lr_r2_score))
print("root mean square error on test set: "+str(lr_rmse))
print()
print("---------------------------------------------------------------")
print()
print()
print("RIDGE REGRESSION:")
print()
print("r2_score on training data: "+str(RR_train_r2_score))
print("r2_score on test set: "+str(RR_r2_score))
print("root mean square error on test set: "+str(RR_rmse))
print()
print()
print("---------------------------------------------------------------")
print()
print("XGBOOST REGRESSION:")
print()
print("r2_score on training data: "+str(xgb_train_r2_score))
print("r2_score on test set: "+str(xgb_r2_score))
print("root mean square error on test set: "+str(xgb_rmse))
print()
print()
print("---------------------------------------------------------------")
print()
print("NEURAL NETWORK REGRESSION:")
print()
print("r2_score on training data: "+str(nn_train_r2_score))
print("r2_score on test set: "+str(nn_r2_score))
print("root mean square error on test set: "+str(nn_rmse))
print()